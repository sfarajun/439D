{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practice","provenance":[],"collapsed_sections":["kxL5oVlOj0yG","_BPUojn71Pbi","lLJF0LJ1cl42","6KmduYKb_OGE","g0cs52sdnnvI","aIIKwIH7ttGF","_5_VFBfdAGtU","zNFtaGe_OcBd","w5ECIHgicxSD","j0K4yBPBILNB","FRM1Jrfqntz7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"kxL5oVlOj0yG"}},{"cell_type":"markdown","source":["## tutorial site: \n","https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e"],"metadata":{"id":"_BPUojn71Pbi"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJXWN-MVX38W","executionInfo":{"status":"ok","timestamp":1649351542486,"user_tz":240,"elapsed":20081,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"39ce1a34-e77c-49a9-dc62-ba90f9173f2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#import necessary libraries\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["data  = pd.read_csv(\"/content/drive/MyDrive/UMD - senior year/spring 2022/439D/project/data.csv\")"],"metadata":{"id":"j2p0QDZ1YpJL","executionInfo":{"status":"ok","timestamp":1649351543749,"user_tz":240,"elapsed":1269,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Feature Engineeing: Fix Up Data"],"metadata":{"id":"lLJF0LJ1cl42"}},{"cell_type":"markdown","source":["## Setup necessary libraries\n","> the imports and stuff"],"metadata":{"id":"6KmduYKb_OGE"}},{"cell_type":"code","source":["#import all necessary libraries for this tutorial\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import collections\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from nltk.corpus import stopwords\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from keras import models\n","from keras import layers\n","import keras"],"metadata":{"id":"EjWwnQx51f3F","executionInfo":{"status":"ok","timestamp":1649351549969,"user_tz":240,"elapsed":6225,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#parameters that will be used in tutorial\n","\n","NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n","VAL_SIZE = 1000  # Size of the validation set\n","NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n","BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n","MAX_LEN = 24  # Maximum number of words in a sequence\n","GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n"],"metadata":{"id":"aiVmZq0v0MqX","executionInfo":{"status":"ok","timestamp":1649351549970,"user_tz":240,"elapsed":28,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"id":"9NeRxCOV305A","executionInfo":{"status":"ok","timestamp":1649351549971,"user_tz":240,"elapsed":26,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"2843a558-ae1f-4c2e-fde2-4076cc9402d7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   no     paper cited_paper  label  \\\n","0   0  A00-1043    A00-2024      0   \n","1   1  H05-1033    A00-2024      0   \n","2   2  I05-2009    A00-2024      0   \n","3   3  I05-2009    A00-2024      0   \n","4   4  I05-2009    A00-2024      0   \n","\n","                                                text  \n","0  We analyzed a set of articles and identified s...  \n","1  Table 3: Example compressions Compression AvgL...  \n","2  5.3 Related works and discussion Our two-step ...  \n","3  (1999) proposed a summarization system based o...  \n","4  We found that the deletion of lead parts did n...  "],"text/html":["\n","  <div id=\"df-cdbed615-e0e6-4606-90ea-c5bba6aad605\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A00-1043</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We analyzed a set of articles and identified s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>H05-1033</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>5.3 Related works and discussion Our two-step ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>(1999) proposed a summarization system based o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We found that the deletion of lead parts did n...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdbed615-e0e6-4606-90ea-c5bba6aad605')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cdbed615-e0e6-4606-90ea-c5bba6aad605 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cdbed615-e0e6-4606-90ea-c5bba6aad605');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## Removing non-alphabetic characters\n","> remove words and characters that aren't useful from sentences in dataset"],"metadata":{"id":"g0cs52sdnnvI"}},{"cell_type":"code","source":["#define functions to things with no sentiment value (irrelevant words)\n","stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n","             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n","             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n","             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n","             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n","             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n","             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n","             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n","             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n","             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n","             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","\n","characters = [\"\"]\n","\n","#function to remove stopwords\n","def remove_stopwords(data):\n","  data['review_without_stopwords'] = data['text'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n","  return data\n","\n","#function to remove non-alphabetical tags\n","def remove_tags(string):\n","    #result = re.sub('[\\d<.*?:>()-,;|/@!#$%^&*~`_=+]','',string)\n","\n","    pattern = re.compile('[\\W_0-9]+')\n","    dirty_list = string.split()\n","    clean_list = [pattern.sub('', word) for word in dirty_list]\n","    result = ' '.join(clean_list)\n","    \n","    # result = re.sub('[\\W_0-9]+','',string)    #see https://blog.finxter.com/how-to-remove-all-non-alphabet-characters-from-a-string/ for explanation\n","    # result = re.sub('  ',' ',result)\n","    return result"],"metadata":{"id":"4Xfp9GbE6s9l","executionInfo":{"status":"ok","timestamp":1649351549971,"user_tz":240,"elapsed":23,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#remove stopwords\n","clean_dataset = remove_stopwords(data)\n","clean_dataset['w/o stopwords or tags']= clean_dataset['review_without_stopwords'].apply(lambda cw : remove_tags(cw))\n","clean_dataset"],"metadata":{"id":"Tlq-liJ-Gpb9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1649351549972,"user_tz":240,"elapsed":23,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"acf7677e-39a2-4c9b-c20f-be37c673e455"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      no     paper cited_paper  label  \\\n","0      0  A00-1043    A00-2024      0   \n","1      1  H05-1033    A00-2024      0   \n","2      2  I05-2009    A00-2024      0   \n","3      3  I05-2009    A00-2024      0   \n","4      4  I05-2009    A00-2024      0   \n","..   ...       ...         ...    ...   \n","994  994  N09-1053    J92-4003      0   \n","995  995  P01-1046    J92-4003      0   \n","996  996  P01-1046    J92-4003      0   \n","997  997  P01-1068    J92-4003      0   \n","998  998  P02-1016    J92-4003      0   \n","\n","                                                  text  \\\n","0    We analyzed a set of articles and identified s...   \n","1    Table 3: Example compressions Compression AvgL...   \n","2    5.3 Related works and discussion Our two-step ...   \n","3    (1999) proposed a summarization system based o...   \n","4    We found that the deletion of lead parts did n...   \n","..                                                 ...   \n","994  While we can only compare class models with wo...   \n","995  (1999) and Lee (1999)) can be generally divide...   \n","996  Classes can be induced directly from the corpu...   \n","997  And we consider that word pairs that have a sm...   \n","998  Words are encoded through an automatic cluster...   \n","\n","                              review_without_stopwords  \\\n","0    We analyzed set articles identified six major ...   \n","1    Table 3: Example compressions Compression AvgL...   \n","2    5.3 Related works discussion Our two-step mode...   \n","3    (1999) proposed summarization system based dra...   \n","4    We found deletion lead parts not occur often s...   \n","..                                                 ...   \n","994  While can compare class models word models lar...   \n","995  (1999) Lee (1999)) can generally divided three...   \n","996  Classes can induced directly corpus (Pereira e...   \n","997  And consider word pairs small distance vectors...   \n","998  Words encoded automatic clustering algorithm (...   \n","\n","                                 w/o stopwords or tags  \n","0    We analyzed set articles identified six major ...  \n","1    Table  Example compressions Compression AvgLen...  \n","2     Related works discussion Our twostep model es...  \n","3     proposed summarization system based draft rev...  \n","4    We found deletion lead parts not occur often s...  \n","..                                                 ...  \n","994  While can compare class models word models lar...  \n","995   Lee  can generally divided three types discou...  \n","996  Classes can induced directly corpus Pereira et...  \n","997  And consider word pairs small distance vectors...  \n","998  Words encoded automatic clustering algorithm B...  \n","\n","[999 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-2be20a87-ae5f-43a8-94c6-857f6605770a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>review_without_stopwords</th>\n","      <th>w/o stopwords or tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A00-1043</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We analyzed a set of articles and identified s...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>H05-1033</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table  Example compressions Compression AvgLen...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>5.3 Related works and discussion Our two-step ...</td>\n","      <td>5.3 Related works discussion Our two-step mode...</td>\n","      <td>Related works discussion Our twostep model es...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>(1999) proposed a summarization system based o...</td>\n","      <td>(1999) proposed summarization system based dra...</td>\n","      <td>proposed summarization system based draft rev...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We found that the deletion of lead parts did n...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>994</th>\n","      <td>994</td>\n","      <td>N09-1053</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>While we can only compare class models with wo...</td>\n","      <td>While can compare class models word models lar...</td>\n","      <td>While can compare class models word models lar...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>995</td>\n","      <td>P01-1046</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>(1999) and Lee (1999)) can be generally divide...</td>\n","      <td>(1999) Lee (1999)) can generally divided three...</td>\n","      <td>Lee  can generally divided three types discou...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>P01-1046</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>Classes can be induced directly from the corpu...</td>\n","      <td>Classes can induced directly corpus (Pereira e...</td>\n","      <td>Classes can induced directly corpus Pereira et...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>997</td>\n","      <td>P01-1068</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>And we consider that word pairs that have a sm...</td>\n","      <td>And consider word pairs small distance vectors...</td>\n","      <td>And consider word pairs small distance vectors...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>998</td>\n","      <td>P02-1016</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>Words are encoded through an automatic cluster...</td>\n","      <td>Words encoded automatic clustering algorithm (...</td>\n","      <td>Words encoded automatic clustering algorithm B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>999 rows × 7 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2be20a87-ae5f-43a8-94c6-857f6605770a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2be20a87-ae5f-43a8-94c6-857f6605770a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2be20a87-ae5f-43a8-94c6-857f6605770a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["clean_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nnYYCly6BWQA","executionInfo":{"status":"ok","timestamp":1649351549972,"user_tz":240,"elapsed":20,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"2268f1e4-8981-464e-9665-45a6e5ba85e2"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      no     paper cited_paper  label  \\\n","0      0  A00-1043    A00-2024      0   \n","1      1  H05-1033    A00-2024      0   \n","2      2  I05-2009    A00-2024      0   \n","3      3  I05-2009    A00-2024      0   \n","4      4  I05-2009    A00-2024      0   \n","..   ...       ...         ...    ...   \n","994  994  N09-1053    J92-4003      0   \n","995  995  P01-1046    J92-4003      0   \n","996  996  P01-1046    J92-4003      0   \n","997  997  P01-1068    J92-4003      0   \n","998  998  P02-1016    J92-4003      0   \n","\n","                                                  text  \\\n","0    We analyzed a set of articles and identified s...   \n","1    Table 3: Example compressions Compression AvgL...   \n","2    5.3 Related works and discussion Our two-step ...   \n","3    (1999) proposed a summarization system based o...   \n","4    We found that the deletion of lead parts did n...   \n","..                                                 ...   \n","994  While we can only compare class models with wo...   \n","995  (1999) and Lee (1999)) can be generally divide...   \n","996  Classes can be induced directly from the corpu...   \n","997  And we consider that word pairs that have a sm...   \n","998  Words are encoded through an automatic cluster...   \n","\n","                              review_without_stopwords  \\\n","0    We analyzed set articles identified six major ...   \n","1    Table 3: Example compressions Compression AvgL...   \n","2    5.3 Related works discussion Our two-step mode...   \n","3    (1999) proposed summarization system based dra...   \n","4    We found deletion lead parts not occur often s...   \n","..                                                 ...   \n","994  While can compare class models word models lar...   \n","995  (1999) Lee (1999)) can generally divided three...   \n","996  Classes can induced directly corpus (Pereira e...   \n","997  And consider word pairs small distance vectors...   \n","998  Words encoded automatic clustering algorithm (...   \n","\n","                                 w/o stopwords or tags  \n","0    We analyzed set articles identified six major ...  \n","1    Table  Example compressions Compression AvgLen...  \n","2     Related works discussion Our twostep model es...  \n","3     proposed summarization system based draft rev...  \n","4    We found deletion lead parts not occur often s...  \n","..                                                 ...  \n","994  While can compare class models word models lar...  \n","995   Lee  can generally divided three types discou...  \n","996  Classes can induced directly corpus Pereira et...  \n","997  And consider word pairs small distance vectors...  \n","998  Words encoded automatic clustering algorithm B...  \n","\n","[999 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-8e196cca-553d-4484-ad01-0846c8d69cfd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>review_without_stopwords</th>\n","      <th>w/o stopwords or tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A00-1043</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We analyzed a set of articles and identified s...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>H05-1033</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table  Example compressions Compression AvgLen...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>5.3 Related works and discussion Our two-step ...</td>\n","      <td>5.3 Related works discussion Our two-step mode...</td>\n","      <td>Related works discussion Our twostep model es...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>(1999) proposed a summarization system based o...</td>\n","      <td>(1999) proposed summarization system based dra...</td>\n","      <td>proposed summarization system based draft rev...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We found that the deletion of lead parts did n...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>994</th>\n","      <td>994</td>\n","      <td>N09-1053</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>While we can only compare class models with wo...</td>\n","      <td>While can compare class models word models lar...</td>\n","      <td>While can compare class models word models lar...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>995</td>\n","      <td>P01-1046</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>(1999) and Lee (1999)) can be generally divide...</td>\n","      <td>(1999) Lee (1999)) can generally divided three...</td>\n","      <td>Lee  can generally divided three types discou...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>P01-1046</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>Classes can be induced directly from the corpu...</td>\n","      <td>Classes can induced directly corpus (Pereira e...</td>\n","      <td>Classes can induced directly corpus Pereira et...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>997</td>\n","      <td>P01-1068</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>And we consider that word pairs that have a sm...</td>\n","      <td>And consider word pairs small distance vectors...</td>\n","      <td>And consider word pairs small distance vectors...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>998</td>\n","      <td>P02-1016</td>\n","      <td>J92-4003</td>\n","      <td>0</td>\n","      <td>Words are encoded through an automatic cluster...</td>\n","      <td>Words encoded automatic clustering algorithm (...</td>\n","      <td>Words encoded automatic clustering algorithm B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>999 rows × 7 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e196cca-553d-4484-ad01-0846c8d69cfd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8e196cca-553d-4484-ad01-0846c8d69cfd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8e196cca-553d-4484-ad01-0846c8d69cfd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Remove data that is too large"],"metadata":{"id":"aIIKwIH7ttGF"}},{"cell_type":"code","source":["# get maxLen for the maximum length of a text \n","maxLen = max(clean_dataset['w/o stopwords or tags'].apply(len))\n","\n","#the row of the maxLen text\n","clean_dataset.loc[clean_dataset['w/o stopwords or tags'].apply(len) == max(clean_dataset['w/o stopwords or tags'].apply(len))]"],"metadata":{"id":"pTcUebCepIyB","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1649351550260,"user_tz":240,"elapsed":23,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"00f245c3-50c0-4c61-82c6-71163888d431"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      no     paper cited_paper  label  \\\n","225  225  W99-0505    C94-2113      0   \n","\n","                                                  text  \\\n","225  Towards a Meaning-Full Comparison of Lexieal R...   \n","\n","                              review_without_stopwords  \\\n","225  Towards Meaning-Full Comparison Lexieal Resour...   \n","\n","                                 w/o stopwords or tags  \n","225  Towards MeaningFull Comparison Lexieal Resourc...  "],"text/html":["\n","  <div id=\"df-0a646a37-dc1e-48bd-b7ce-4e655b6b96b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>review_without_stopwords</th>\n","      <th>w/o stopwords or tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>225</th>\n","      <td>225</td>\n","      <td>W99-0505</td>\n","      <td>C94-2113</td>\n","      <td>0</td>\n","      <td>Towards a Meaning-Full Comparison of Lexieal R...</td>\n","      <td>Towards Meaning-Full Comparison Lexieal Resour...</td>\n","      <td>Towards MeaningFull Comparison Lexieal Resourc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a646a37-dc1e-48bd-b7ce-4e655b6b96b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0a646a37-dc1e-48bd-b7ce-4e655b6b96b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0a646a37-dc1e-48bd-b7ce-4e655b6b96b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# sizes of texts \n","clean_dataset['w/o stopwords or tags'].apply(len).sort_values()"],"metadata":{"id":"ZvZBe8J2uu0i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649351550261,"user_tz":240,"elapsed":21,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"8ecbd307-a9d4-4197-e627-b00959a66c94"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["144        9\n","127       11\n","393       12\n","704       13\n","642       13\n","       ...  \n","637      781\n","321      835\n","908     1717\n","909     3000\n","225    22682\n","Name: w/o stopwords or tags, Length: 999, dtype: int64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#try removing thousand word lines from dataset to make training easier\n","smaller_clean_dataset = clean_dataset.loc[clean_dataset['w/o stopwords or tags'].apply(len) < 1000]\n","smaller_clean_dataset['w/o stopwords or tags'].apply(len).sort_values()"],"metadata":{"id":"IuizTg6gjNf6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649351550261,"user_tz":240,"elapsed":19,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"d81e70f2-78ee-4f67-fcb9-2b5def425a99"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["144      9\n","127     11\n","393     12\n","704     13\n","642     13\n","      ... \n","896    596\n","907    730\n","561    749\n","637    781\n","321    835\n","Name: w/o stopwords or tags, Length: 996, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#redefine maxlen as length of new longest sentence\n","maxLen = max(smaller_clean_dataset['w/o stopwords or tags'].apply(len))"],"metadata":{"id":"zTYu-8LfuuN6","executionInfo":{"status":"ok","timestamp":1649351550262,"user_tz":240,"elapsed":19,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## One hot encode labels"],"metadata":{"id":"_5_VFBfdAGtU"}},{"cell_type":"code","source":["#copy original dataset as backup if mess up\n","smaller_clean_dataset_orig = smaller_clean_dataset.copy()\n","\n","#add categorical labels for label to original dataset \n","smaller_clean_dataset = pd.concat([smaller_clean_dataset, pd.get_dummies(smaller_clean_dataset['label'], prefix='label_')], axis=1)\n","smaller_clean_dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"qAuJ9ErWC0U7","executionInfo":{"status":"ok","timestamp":1649351550262,"user_tz":240,"elapsed":18,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"7c17e375-7650-4e60-928c-cc9ce375ef6c"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   no     paper cited_paper  label  \\\n","0   0  A00-1043    A00-2024      0   \n","1   1  H05-1033    A00-2024      0   \n","2   2  I05-2009    A00-2024      0   \n","3   3  I05-2009    A00-2024      0   \n","4   4  I05-2009    A00-2024      0   \n","\n","                                                text  \\\n","0  We analyzed a set of articles and identified s...   \n","1  Table 3: Example compressions Compression AvgL...   \n","2  5.3 Related works and discussion Our two-step ...   \n","3  (1999) proposed a summarization system based o...   \n","4  We found that the deletion of lead parts did n...   \n","\n","                            review_without_stopwords  \\\n","0  We analyzed set articles identified six major ...   \n","1  Table 3: Example compressions Compression AvgL...   \n","2  5.3 Related works discussion Our two-step mode...   \n","3  (1999) proposed summarization system based dra...   \n","4  We found deletion lead parts not occur often s...   \n","\n","                               w/o stopwords or tags  label__-1  label__0  \\\n","0  We analyzed set articles identified six major ...          0         1   \n","1  Table  Example compressions Compression AvgLen...          0         1   \n","2   Related works discussion Our twostep model es...          0         1   \n","3   proposed summarization system based draft rev...          0         1   \n","4  We found deletion lead parts not occur often s...          0         1   \n","\n","   label__1  \n","0         0  \n","1         0  \n","2         0  \n","3         0  \n","4         0  "],"text/html":["\n","  <div id=\"df-4adb6790-f700-4988-83dd-02d06f7eb746\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>review_without_stopwords</th>\n","      <th>w/o stopwords or tags</th>\n","      <th>label__-1</th>\n","      <th>label__0</th>\n","      <th>label__1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A00-1043</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We analyzed a set of articles and identified s...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","      <td>We analyzed set articles identified six major ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>H05-1033</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table 3: Example compressions Compression AvgL...</td>\n","      <td>Table  Example compressions Compression AvgLen...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>5.3 Related works and discussion Our two-step ...</td>\n","      <td>5.3 Related works discussion Our two-step mode...</td>\n","      <td>Related works discussion Our twostep model es...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>(1999) proposed a summarization system based o...</td>\n","      <td>(1999) proposed summarization system based dra...</td>\n","      <td>proposed summarization system based draft rev...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I05-2009</td>\n","      <td>A00-2024</td>\n","      <td>0</td>\n","      <td>We found that the deletion of lead parts did n...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","      <td>We found deletion lead parts not occur often s...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4adb6790-f700-4988-83dd-02d06f7eb746')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4adb6790-f700-4988-83dd-02d06f7eb746 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4adb6790-f700-4988-83dd-02d06f7eb746');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["smaller_clean_dataset.iloc[:, -3:].to_numpy().shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiQVK-f3HKOm","executionInfo":{"status":"ok","timestamp":1649351550263,"user_tz":240,"elapsed":18,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"2d989d73-ded5-495d-ca3f-a02d0f9bc044"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(996, 3)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["## Even out dataset by label\n","> Scale up the non-neutral sentiment data\n"],"metadata":{"id":"zNFtaGe_OcBd"}},{"cell_type":"code","source":["#scale up the non-neutral sentiment data\n","smaller_clean_dataset['label'].value_counts()\n","\n","# add all the 1 sentiment data in 11x + first 27 \n","# add all the 1 sentiment data in 42x + first 14"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z5-VQ8VPGvC","executionInfo":{"status":"ok","timestamp":1649351550263,"user_tz":240,"elapsed":17,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"37d2086a-aae8-4c27-e10d-8c22d991c5e0"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":[" 0    896\n"," 1     79\n","-1     21\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# POSITIVE sentiment data -----\n","#repeat positive sentiment data 10 times\n","vertical_concat = smaller_clean_dataset.loc[smaller_clean_dataset['label'] == 1]\n","for i in range(9):\n","  vertical_concat = pd.concat([vertical_concat, smaller_clean_dataset.loc[smaller_clean_dataset['label'] == 1]], axis=0)\n","\n","#add first 27 entries to that dataset\n","vertical_concat = pd.concat([vertical_concat, vertical_concat.iloc[:27]], axis=0)\n","\n","\n","\n","# NEGATIVE sentiment data -----\n","#repeat positive sentiment data 40 times\n","vertical_concat2 = smaller_clean_dataset.loc[smaller_clean_dataset['label'] == -1]\n","for i in range(40):\n","  vertical_concat2 = pd.concat([vertical_concat2, smaller_clean_dataset.loc[smaller_clean_dataset['label'] == -1]], axis=0)\n","\n","#add first 27 entries to that dataset\n","vertical_concat2 = pd.concat([vertical_concat2, vertical_concat2.iloc[:14]], axis=0)\n","\n","\n","\n","print('new 1 label shape', vertical_concat.shape)\n","print('new -1 label shape', vertical_concat2.shape)\n","vertical_concat.head(3)\n","# vertical_concat2.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"id":"A4ME5mxiSXse","executionInfo":{"status":"ok","timestamp":1649351550528,"user_tz":240,"elapsed":280,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"222441d5-79e5-4c9f-9662-d57c7461e470"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["new 1 label shape (817, 10)\n","new -1 label shape (875, 10)\n"]},{"output_type":"execute_result","data":{"text/plain":["    no     paper cited_paper  label  \\\n","6    6  I08-2101    A00-2024      1   \n","9    9  J02-4005    A00-2024      1   \n","21  21  W03-1102    A00-2024      1   \n","\n","                                                 text  \\\n","6   al., 1994), compression of sentences with Auto...   \n","9   But in fact, the issue of editing in text summ...   \n","21  The recent approach for editing extracted text...   \n","\n","                             review_without_stopwords  \\\n","6   al., 1994), compression sentences Automatic Tr...   \n","9   But fact, issue editing text summarization usu...   \n","21  The recent approach editing extracted text spa...   \n","\n","                                w/o stopwords or tags  label__-1  label__0  \\\n","6   al  compression sentences Automatic Translatio...          0         0   \n","9   But fact issue editing text summarization usua...          0         0   \n","21  The recent approach editing extracted text spa...          0         0   \n","\n","    label__1  \n","6          1  \n","9          1  \n","21         1  "],"text/html":["\n","  <div id=\"df-4e84cd10-77ce-4870-83c1-4ffda83b8969\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>paper</th>\n","      <th>cited_paper</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>review_without_stopwords</th>\n","      <th>w/o stopwords or tags</th>\n","      <th>label__-1</th>\n","      <th>label__0</th>\n","      <th>label__1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>I08-2101</td>\n","      <td>A00-2024</td>\n","      <td>1</td>\n","      <td>al., 1994), compression of sentences with Auto...</td>\n","      <td>al., 1994), compression sentences Automatic Tr...</td>\n","      <td>al  compression sentences Automatic Translatio...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>J02-4005</td>\n","      <td>A00-2024</td>\n","      <td>1</td>\n","      <td>But in fact, the issue of editing in text summ...</td>\n","      <td>But fact, issue editing text summarization usu...</td>\n","      <td>But fact issue editing text summarization usua...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>W03-1102</td>\n","      <td>A00-2024</td>\n","      <td>1</td>\n","      <td>The recent approach for editing extracted text...</td>\n","      <td>The recent approach editing extracted text spa...</td>\n","      <td>The recent approach editing extracted text spa...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e84cd10-77ce-4870-83c1-4ffda83b8969')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e84cd10-77ce-4870-83c1-4ffda83b8969 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e84cd10-77ce-4870-83c1-4ffda83b8969');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#save old dataframe \n","old_dataframe = smaller_clean_dataset.copy()\n","\n","#concatenate the dataframes\n","non_neutral_data = pd.concat([vertical_concat, vertical_concat2], axis=0)\n","smaller_clean_dataset = pd.concat([smaller_clean_dataset, non_neutral_data], axis=0)\n","\n","#shuffle data\n","from sklearn.utils import shuffle\n","smaller_clean_dataset = shuffle(smaller_clean_dataset, random_state=0)\n","\n","smaller_clean_dataset['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rb4XR5LOWB0z","executionInfo":{"status":"ok","timestamp":1649351550529,"user_tz":240,"elapsed":16,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"e2a1de93-8c9d-42c8-d12e-0922b8f08dd6"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1    896\n"," 0    896\n"," 1    896\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# Train and Test Model"],"metadata":{"id":"w5ECIHgicxSD"}},{"cell_type":"markdown","source":["## Testing with multiple outputs"],"metadata":{"id":"j0K4yBPBILNB"}},{"cell_type":"code","source":["# split into test and training data\n","X_train, X_test,Y_train, Y_test = train_test_split(smaller_clean_dataset['w/o stopwords or tags'], smaller_clean_dataset.iloc[:, -3:].to_numpy(), test_size=0.2, random_state = 45)"],"metadata":{"id":"6kzXRcOQILNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Y_train 3D vector\n","#Y_train\n","\n","#Y_train sentiment\n","(np.argmax(Y_train, axis=1)[:300] - 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rN3A1XB3eO68","executionInfo":{"status":"ok","timestamp":1649273107968,"user_tz":240,"elapsed":117,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"da04c1c9-f307-42c3-e37d-63e76f538006"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1,  1, -1,  1,  1, -1,  1,  0,  0,  0,  1,  0, -1,  1,  0,  0,  1,\n","        1,  0,  0,  1,  0, -1,  1,  0, -1,  0,  1,  0,  1,  1, -1,  0,  1,\n","        1, -1, -1,  0, -1,  0,  1,  1,  0, -1,  0,  1,  0,  1,  1,  1, -1,\n","       -1, -1,  0, -1,  0,  1,  1,  0,  1, -1,  1, -1,  0,  1,  0, -1,  1,\n","        0,  1,  1, -1, -1,  0,  1, -1, -1, -1, -1, -1,  0, -1,  0,  0, -1,\n","       -1, -1,  0,  1,  1,  1,  1, -1, -1, -1, -1,  0,  1,  0, -1,  1, -1,\n","        0, -1, -1,  0,  1, -1,  0, -1,  0,  0,  0,  1,  0, -1, -1,  1, -1,\n","       -1,  0, -1,  0,  0,  1, -1, -1, -1, -1,  0, -1,  1,  0,  1,  1, -1,\n","       -1,  1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1, -1,  0, -1,  1,\n","       -1,  0,  1,  1,  1,  1,  0,  0, -1, -1,  1,  1,  0,  0,  0,  1,  0,\n","        0,  0,  0,  1,  1,  1, -1,  0,  1, -1, -1, -1,  1,  1,  0,  1, -1,\n","        1,  1,  1,  0,  0,  0,  0, -1,  1, -1,  1, -1,  1,  1,  1,  1,  0,\n","        0,  1,  1,  1,  1, -1, -1, -1,  0,  0,  1, -1, -1,  0,  1, -1,  0,\n","        1, -1,  1, -1,  0,  0,  0, -1, -1,  1,  1,  0, -1,  1,  1, -1,  1,\n","        1,  0,  0, -1,  0,  1, -1,  0,  0, -1,  1, -1,  0, -1,  1, -1, -1,\n","       -1,  0, -1,  0, -1,  0, -1, -1,  1,  0,  0,  1,  0, -1, -1, -1,  1,\n","       -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  0,  1,  0,  0,\n","        0,  1, -1,  0, -1,  1, -1, -1, -1,  1,  1])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["# create list of unique words in sentences\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train)\n","\n","#creates dictionary of each {word: index}\n","words_to_index = tokenizer.word_index\n","words_to_index"],"metadata":{"id":"CkCfFj2pILNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function to read GloCe Vector file\n","def read_glove_vector(glove_vec):\n","  with open(glove_vec, 'r', encoding='UTF-8') as f:\n","    words = set()\n","    word_to_vec_map = {}\n","    for line in f:\n","      w_line = line.split()\n","      curr_word = w_line[0]\n","      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n","\n","\n","\n","  return word_to_vec_map"],"metadata":{"id":"_aaj6FllILNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read in GloVe vector from Google Drive (premade mapping of words to be used for sentiment analysis)\n","word_to_vec_map = read_glove_vector('/content/drive/MyDrive/UMD - senior year/spring 2022/439D/project/glove.6B.50d.txt')"],"metadata":{"id":"K3OJwRicILND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","\n","\n","# create embedding matrix (all words in GloVe vector assigned to correct value matrix, all others assigend to 0 vector)\n","vocab_len = len(words_to_index) + 1\n","embed_vector_len = word_to_vec_map['moon'].shape[0]\n","\n","emb_matrix = np.zeros((vocab_len, embed_vector_len))\n","\n","\n","#get glove coordinates of words that are in BOTH glove list and in training data sentences \n","for word, index in words_to_index.items():\n","  embedding_vector = word_to_vec_map.get(word)\n","  if embedding_vector is not None:\n","    print(word)\n","    emb_matrix[index, :] = embedding_vector\n","\n","embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"],"metadata":{"id":"SXC7zvfSILND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.datasets import imdb\n","import pandas as pd\n","import numpy as np\n","from keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n","from keras.layers.embeddings import Embedding\n","from keras.models import Model\n","import string\n","import re\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.preprocessing.sequence import pad_sequences\n","import keras\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"9I6GWlwgILND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create model architecture\n","def imdb_rating(input_shape):\n","\n","  X_indices = Input(input_shape)\n","\n","  embeddings = embedding_layer(X_indices)\n","\n","  X = LSTM(128, return_sequences=True)(embeddings)\n","\n","  X = Dropout(0.6)(X)\n","\n","  X = LSTM(128, return_sequences=True)(X)\n","\n","  X = Dropout(0.6)(X)\n","\n","  X = LSTM(128)(X)\n","\n","  #X = Dense(1, activation='sigmoid')(X)\n","\n","  X = Dense(3, activation='softmax')(X)\n","\n","  model = Model(inputs=X_indices, outputs=X)\n","\n","  return model"],"metadata":{"id":"JDA-0ehuILND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#tokenizer converts words in the sentence into its corresponding indexe \n","print(X_train.iloc[-1])  \n","print(tokenizer.texts_to_sequences(X_train)[-1])\n","print('the word since, the first word of the last sentence in the dataset has index: ', words_to_index['regardless'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649272418609,"user_tz":240,"elapsed":517,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"10132e18-bd51-43d1-f13a-312e2d5a90b9","id":"2D90bAcAILNE"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We used pointwise mutual information PMI Church Hanks  obtain distances\n","[41, 4, 99, 23, 5, 467, 3, 9, 1228, 1645]\n","the word since, the first word of the last sentence in the dataset has index:  1976\n"]}]},{"cell_type":"code","source":["#convert training data sentence words into its corresponding indexes\n","X_train_indices = tokenizer.texts_to_sequences(X_train)\n","\n","#pad sentences of indexes so all same length\n","X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n","print('the shape of the new training data is 796 samples, each with 835 words (bc of padding): ', X_train_indices.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649272418609,"user_tz":240,"elapsed":8,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"b6b61c5e-ad5c-47c8-ebea-f386c755e7c0","id":"HvAfR1D5ILNE"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the shape of the new training data is 796 samples, each with 835 words (bc of padding):  (2150, 835)\n"]}]},{"cell_type":"code","source":["#train the model\n","model = imdb_rating((maxLen,))\n","\n","model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_indices, Y_train, batch_size=256, epochs=15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649274884884,"user_tz":240,"elapsed":153537,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"f1caccfb-68eb-4e9a-e6e8-c57449a7e5b1","id":"-RBRdqh-ILNE"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","9/9 [==============================] - 15s 642ms/step - loss: 0.6664 - accuracy: 0.3442\n","Epoch 2/15\n","9/9 [==============================] - 6s 646ms/step - loss: 0.6407 - accuracy: 0.3312\n","Epoch 3/15\n","9/9 [==============================] - 6s 642ms/step - loss: 0.6370 - accuracy: 0.3442\n","Epoch 4/15\n","9/9 [==============================] - 6s 644ms/step - loss: 0.6370 - accuracy: 0.3442\n","Epoch 5/15\n","9/9 [==============================] - 6s 642ms/step - loss: 0.6368 - accuracy: 0.3423\n","Epoch 6/15\n","9/9 [==============================] - 6s 642ms/step - loss: 0.6367 - accuracy: 0.3247\n","Epoch 7/15\n","9/9 [==============================] - 6s 642ms/step - loss: 0.6366 - accuracy: 0.3451\n","Epoch 8/15\n","9/9 [==============================] - 6s 645ms/step - loss: 0.6367 - accuracy: 0.3442\n","Epoch 9/15\n","9/9 [==============================] - 6s 640ms/step - loss: 0.6368 - accuracy: 0.3442\n","Epoch 10/15\n","9/9 [==============================] - 6s 637ms/step - loss: 0.6366 - accuracy: 0.3330\n","Epoch 11/15\n","9/9 [==============================] - 6s 639ms/step - loss: 0.6366 - accuracy: 0.3447\n","Epoch 12/15\n","9/9 [==============================] - 6s 641ms/step - loss: 0.6366 - accuracy: 0.3442\n","Epoch 13/15\n","9/9 [==============================] - 6s 645ms/step - loss: 0.6366 - accuracy: 0.3442\n","Epoch 14/15\n","9/9 [==============================] - 6s 637ms/step - loss: 0.6366 - accuracy: 0.3442\n","Epoch 15/15\n","9/9 [==============================] - 6s 642ms/step - loss: 0.6366 - accuracy: 0.3437\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7420f59710>"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["#convert the testing data sentence words into its corresponding indexes\n","X_test_indices = tokenizer.texts_to_sequences(X_test)\n","\n","#pad sentences of indexes so all same length\n","X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n","print('the shape of the new training data is 200 samples, each with 835 words (bc of padding): ', X_test_indices.shape)\n","\n","\n","model.evaluate(X_test_indices, Y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649274889102,"user_tz":240,"elapsed":4229,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"6ec5d88c-3b73-40fc-f4ae-f437769828c0","id":"IP_I7Z2xILNF"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the shape of the new training data is 200 samples, each with 835 words (bc of padding):  (538, 835)\n","17/17 [==============================] - 4s 181ms/step - loss: 0.6380 - accuracy: 0.2900\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.637998640537262, 0.2899628281593323]"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["my_predictions = model.predict(X_test_indices)"],"metadata":{"id":"lB2oQULqILNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_predictions[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649272892022,"user_tz":240,"elapsed":231,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"42f43277-a6c1-43df-e99e-45f6a82a75e8","id":"7xpVE26aILNF"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.32146624],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.32146624],\n","       [0.36134157, 0.3171923 , 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.32146624],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134157, 0.3171923 , 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134157, 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134157, 0.3171923 , 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.32146624],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719223, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.3613415 , 0.31719226, 0.32146618],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ],\n","       [0.36134154, 0.31719226, 0.3214662 ]], dtype=float32)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["#take max of the softmax output\n","my_predictions_final = np.argmax(my_predictions, axis=1)\n","\n","#map values back to (-1,0,1)\n","my_predictions_final - 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sx0PvSoZJZYU","executionInfo":{"status":"ok","timestamp":1649272701555,"user_tz":240,"elapsed":15,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"70f37878-63b6-4d4d-8e49-f7e1035f7785"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n","       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["np.argmax(Y_test, axis=1)"],"metadata":{"id":"Vg15f33LJ6L8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649272774198,"user_tz":240,"elapsed":121,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"6cb125db-ffb1-4b76-c9ae-636e18f3a846"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, 2,\n","       1, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2,\n","       1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 1, 1, 0, 2, 0, 2, 1, 2, 1, 2, 0, 0,\n","       1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 2, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 1,\n","       0, 0, 2, 1, 0, 0, 2, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 1,\n","       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1,\n","       0, 2, 2, 1, 2, 1, 2, 2, 1, 1, 0, 2, 2, 2, 2, 2, 1, 0, 1, 2, 0, 1,\n","       1, 0, 2, 1, 1, 1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 1, 2, 2, 2, 1, 0, 0,\n","       0, 2, 0, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2,\n","       1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 2, 2, 1, 0,\n","       2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 2, 1, 0, 0, 1, 1, 2, 2, 2, 2, 1,\n","       1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0,\n","       2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 0,\n","       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 0, 1,\n","       1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 1, 1,\n","       0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 0, 2, 0, 1, 1, 2, 0, 0, 1,\n","       0, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 0, 1, 0,\n","       1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0,\n","       2, 0, 2, 1, 0, 2, 1, 0, 0, 1, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1,\n","       2, 1, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2, 1, 0, 0,\n","       0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2,\n","       0, 2, 0, 1, 2, 0, 2, 0, 2, 2, 1, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 1,\n","       1, 1, 0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 2, 2, 1, 2, 2, 0, 0, 2, 0,\n","       1, 0, 0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2,\n","       1, 1, 0, 1, 0, 1, 2, 1, 1, 1])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["# --------Old\n","> dont use this last part. Only keepig it for research reasons"],"metadata":{"id":"FRM1Jrfqntz7"}},{"cell_type":"code","source":["# split into test and training data\n","X_train, X_test,Y_train, Y_test = train_test_split(smaller_clean_dataset['w/o stopwords or tags'], smaller_clean_dataset['label'], test_size=0.2, random_state = 45)"],"metadata":{"id":"gD1ibjxCS_n8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create list of unique words in sentences\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train)\n","\n","#creates dictionary of each {word: index}\n","words_to_index = tokenizer.word_index\n","words_to_index"],"metadata":{"id":"MXux9vwUTF87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function to read GloCe Vector file\n","def read_glove_vector(glove_vec):\n","  with open(glove_vec, 'r', encoding='UTF-8') as f:\n","    words = set()\n","    word_to_vec_map = {}\n","    for line in f:\n","      w_line = line.split()\n","      curr_word = w_line[0]\n","      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n","\n","\n","\n","  return word_to_vec_map"],"metadata":{"id":"5sUYAwLcTQTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read in GloVe vector from Google Drive (premade mapping of words to be used for sentiment analysis)\n","word_to_vec_map = read_glove_vector('/content/drive/MyDrive/UMD - senior year/spring 2022/439D/project/glove.6B.50d.txt')"],"metadata":{"id":"bKL3mwqoEbmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ajxppUm9naUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","\n","\n","# create embedding matrix (all words in GloVe vector assigned to correct value matrix, all others assigend to 0 vector)\n","vocab_len = len(words_to_index) + 1\n","embed_vector_len = word_to_vec_map['moon'].shape[0]\n","\n","emb_matrix = np.zeros((vocab_len, embed_vector_len))\n","\n","\n","#get glove coordinates of words that are in BOTH glove list and in training data sentences \n","for word, index in words_to_index.items():\n","  embedding_vector = word_to_vec_map.get(word)\n","  if embedding_vector is not None:\n","    emb_matrix[index, :] = embedding_vector\n","\n","embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"],"metadata":{"id":"dDR2aGCvpwr0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.datasets import imdb\n","import pandas as pd\n","import numpy as np\n","from keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n","from keras.layers.embeddings import Embedding\n","from keras.models import Model\n","import string\n","import re\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.preprocessing.sequence import pad_sequences\n","import keras\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"hXCWYlRH0N3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create model architecture\n","def imdb_rating(input_shape):\n","\n","  X_indices = Input(input_shape)\n","\n","  embeddings = embedding_layer(X_indices)\n","\n","  X = LSTM(128, return_sequences=True)(embeddings)\n","\n","  X = Dropout(0.6)(X)\n","\n","  X = LSTM(128, return_sequences=True)(X)\n","\n","  X = Dropout(0.6)(X)\n","\n","  X = LSTM(128)(X)\n","\n","  #X = Dense(1, activation='sigmoid')(X)\n","\n","  X = Dense(3, activation='softmax')(X)\n","\n","  model = Model(inputs=X_indices, outputs=X)\n","\n","  return model"],"metadata":{"id":"vC7ntmuVuOPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#tokenizer converts words in the sentence into its corresponding indexe \n","print(X_train.iloc[-1])  \n","print(tokenizer.texts_to_sequences(X_train)[-1])\n","print('the word since, the first word of the last sentence in the dataset has index: ', words_to_index['regardless'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rmly2plyq00p","executionInfo":{"status":"ok","timestamp":1649182905776,"user_tz":240,"elapsed":417,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"aa944304-7c79-4792-f62c-9398aad9f3f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Regardless whether takes form dictionaries Lesk  Guthrie et al  Dagan Itai Schwall  Karov Edelman  thesauri Yarowsky  Walker Amsler  bilingual corpora Brown et al  Church Gale  handlabeled training sets Hearst  Leacock Towell Voorhees  Niwa Nitta  Bruce Wiebe  providing information sense definitions can considerable burden\n","[3871, 472, 748, 229, 484, 906, 807, 2, 1, 125, 1738, 3872, 3873, 3874, 674, 154, 3875, 3876, 568, 99, 26, 2, 1, 3, 269, 3877, 35, 265, 1833, 709, 1834, 980, 1771, 1772, 1835, 1153, 3878, 5, 140, 599, 17, 584, 3879]\n","the word since, the first word of the last sentence in the dataset has index:  3871\n"]}]},{"cell_type":"code","source":["#convert training data sentence words into its corresponding indexes\n","X_train_indices = tokenizer.texts_to_sequences(X_train)\n","\n","#pad sentences of indexes so all same length\n","X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n","print('the shape of the new training data is 796 samples, each with 835 words (bc of padding): ', X_train_indices.shape)"],"metadata":{"id":"2Wa3GFZozF2x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649182908394,"user_tz":240,"elapsed":6,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"72ce8e11-e22b-4cd3-adbc-0948110930f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the shape of the new training data is 796 samples, each with 835 words (bc of padding):  (796, 835)\n"]}]},{"cell_type":"code","source":["#train the model\n","model = imdb_rating((maxLen,))\n","\n","model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_indices, Y_train, batch_size=64, epochs=15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"id":"rF-_RMpzzIsA","executionInfo":{"status":"error","timestamp":1649182917575,"user_tz":240,"elapsed":5986,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"8623069c-1d11-4412-d93c-3bf0dfea741d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-3e06de715ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 3) vs (None, 1)).\n"]}]},{"cell_type":"code","source":["#convert the testing data sentence words into its corresponding indexes\n","X_test_indices = tokenizer.texts_to_sequences(X_test)\n","\n","#pad sentences of indexes so all same length\n","X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n","print('the shape of the new training data is 200 samples, each with 835 words (bc of padding): ', X_test_indices.shape)\n","\n","\n","model.evaluate(X_test_indices, Y_test)"],"metadata":{"id":"NSvxbmX_0d_v","colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"status":"error","timestamp":1648828241896,"user_tz":240,"elapsed":564,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"f2a1f55e-1b00-4a11-9d77-780b1e845135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the shape of the new training data is 200 samples, each with 835 words (bc of padding):  (200, 835)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-7aa571050846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1473, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 3) vs (None, 1)).\n"]}]},{"cell_type":"code","source":["my_predictions = model.predict(X_test_indices)"],"metadata":{"id":"BXYwjVRm0dwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dF-6BmGUxBSR","executionInfo":{"status":"ok","timestamp":1648827309770,"user_tz":240,"elapsed":118,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"a31249dd-84cc-47c4-f01f-aae19c0bf255"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476179],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476178],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476179],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476178],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476176],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177],\n","       [0.05476177]], dtype=float32)"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["smaller_clear_dataset['label'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dyh992k7xPR4","executionInfo":{"status":"ok","timestamp":1648827410406,"user_tz":240,"elapsed":100,"user":{"displayName":"sahar farajun","userId":"08766982238663737774"}},"outputId":"dbc0b50b-6d0e-461f-a594-0f6c653cd14c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0,  1, -1])"]},"metadata":{},"execution_count":107}]}]}